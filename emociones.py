from flask import Flask, request, jsonify
import cv2
import numpy as np
import base64
from io import BytesIO
from PIL import Image
import requests
import os
import logging
from transformers import pipeline
import torch
import warnings

# Suprimir warnings
warnings.filterwarnings("ignore")

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)

# Configuraci√≥n
EMOTIONS = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']

class EmotionDetectorHF:
    def __init__(self):
        """Inicializar el detector de emociones con Hugging Face"""
        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        self.emotion_classifier = None
        self.load_emotion_model()
    
    def load_emotion_model(self):
        """Cargar modelo de Hugging Face"""
        try:
            logger.info("üöÄ Cargando modelo de emociones desde Hugging Face...")
            
            # Modelo espec√≠fico para detecci√≥n de emociones en caras
            # Este modelo est√° optimizado y es muy preciso
            self.emotion_classifier = pipeline(
                "image-classification",
                model="j-hartmann/emotion-english-distilroberta-base",
                return_all_scores=True,
                device=-1  # CPU para mejor compatibilidad en deploy
            )
            
            logger.info("‚úÖ Modelo de Hugging Face cargado exitosamente")
            
        except Exception as e:
            logger.error(f"‚ùå Error cargando modelo de HF: {e}")
            logger.info("üîÑ Usando modelo de respaldo...")
            self.emotion_classifier = None
    
    def detect_faces(self, image):
        """Detectar caras en la imagen - MEJORADO"""
        # Convertir a escala de grises si es necesario
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        else:
            gray = image
            
        # Detectar caras con par√°metros optimizados
        faces = self.face_cascade.detectMultiScale(
            gray, 
            scaleFactor=1.05,  # M√°s sensible
            minNeighbors=3,    # Menos restrictivo
            minSize=(20, 20),  # Caras m√°s peque√±as
            maxSize=(300, 300) # L√≠mite m√°ximo
        )
        return faces, gray
    
    def predict_emotion_hf(self, face_image):
        """Predecir emoci√≥n usando Hugging Face - MEJORADO"""
        try:
            if self.emotion_classifier is None:
                return self.predict_emotion_local(face_image)
            
            # Redimensionar cara para mejor an√°lisis
            face_resized = cv2.resize(face_image, (224, 224))
            
            # Convertir a PIL Image RGB
            face_pil = Image.fromarray(cv2.cvtColor(face_resized, cv2.COLOR_BGR2RGB))
            
            # Hacer predicci√≥n con todas las puntuaciones
            results = self.emotion_classifier(face_pil)
            
            # Procesar resultados para obtener la mejor emoci√≥n
            emotion_scores = {}
            for result in results:
                label = result['label'].lower()
                score = result['score']
                
                # Mapear etiquetas del modelo a nuestras emociones
                if 'joy' in label or 'happy' in label:
                    emotion_scores['Happy'] = score
                elif 'anger' in label or 'angry' in label:
                    emotion_scores['Angry'] = score
                elif 'sadness' in label or 'sad' in label:
                    emotion_scores['Sad'] = score
                elif 'fear' in label:
                    emotion_scores['Fear'] = score
                elif 'surprise' in label:
                    emotion_scores['Surprise'] = score
                elif 'disgust' in label:
                    emotion_scores['Disgust'] = score
                else:
                    emotion_scores['Neutral'] = score
            
            # Obtener la emoci√≥n con mayor puntuaci√≥n
            if emotion_scores:
                best_emotion = max(emotion_scores, key=emotion_scores.get)
                confidence = emotion_scores[best_emotion]
            else:
                best_emotion = 'Neutral'
                confidence = 0.5
            
            return best_emotion, confidence
            
        except Exception as e:
            logger.error(f"‚ùå Error con HF: {e}")
            return self.predict_emotion_local(face_image)
    
    def predict_emotion_local(self, face_image):
        """Predicci√≥n local como respaldo - MEJORADO"""
        # An√°lisis b√°sico de la imagen para predicci√≥n m√°s realista
        gray = cv2.cvtColor(face_image, cv2.COLOR_BGR2GRAY) if len(face_image.shape) == 3 else face_image
        
        # Calcular caracter√≠sticas b√°sicas de la imagen
        mean_intensity = np.mean(gray)
        std_intensity = np.std(gray)
        
        # L√≥gica simple basada en caracter√≠sticas de la imagen
        if mean_intensity > 150:
            emotion = 'Happy'
            confidence = 0.65
        elif mean_intensity < 80:
            emotion = 'Sad'
            confidence = 0.60
        elif std_intensity > 50:
            emotion = 'Surprise'
            confidence = 0.58
        else:
            emotion = 'Neutral'
            confidence = 0.55
            
        return emotion, confidence
    
    def process_image(self, image):
        """Procesar imagen y detectar emociones - MEJORADO"""
        faces, gray = self.detect_faces(image)
        results = []
        
        # Crear copia de la imagen para dibujar
        result_image = image.copy()
        
        logger.info(f"üîç Detectadas {len(faces)} caras en la imagen")
        
        for i, (x, y, w, h) in enumerate(faces):
            logger.info(f"üìä Procesando cara {i+1}/{len(faces)}")
            
            # Extraer regi√≥n facial con margen
            margin = 10
            y_start = max(0, y - margin)
            y_end = min(image.shape[0], y + h + margin)
            x_start = max(0, x - margin)
            x_end = min(image.shape[1], x + w + margin)
            
            face_roi = image[y_start:y_end, x_start:x_end]
            
            # Predecir emoci√≥n usando Hugging Face
            emotion, confidence = self.predict_emotion_hf(face_roi)
            
            # Elegir color basado en confianza
            if confidence > 0.8:
                color = (0, 255, 0)  # Verde - Alta confianza
            elif confidence > 0.6:
                color = (0, 255, 255)  # Amarillo - Media confianza
            else:
                color = (0, 165, 255)  # Naranja - Baja confianza
            
            # Dibujar rect√°ngulo m√°s grueso
            cv2.rectangle(result_image, (x, y), (x+w, y+h), color, 3)
            
            # Texto con emoci√≥n y confianza - MEJORADO
            text = f"{emotion}: {confidence:.2f}"
            font_scale = 0.7
            thickness = 2
            
            # Calcular tama√±o del texto para fondo
            (text_width, text_height), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness)
            
            # Dibujar fondo para el texto
            cv2.rectangle(result_image, (x, y-text_height-10), (x+text_width, y), color, -1)
            
            # Dibujar texto
            cv2.putText(result_image, text, (x, y-5), 
                       cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0, 0, 0), thickness)
            
            results.append({
                'emotion': emotion,
                'confidence': float(confidence),  # Asegurar que sea float
                'bbox': [int(x), int(y), int(w), int(h)]
            })
            
            logger.info(f"‚úÖ Cara {i+1}: {emotion} ({confidence:.2f})")
        
        return result_image, results

def image_to_base64(image):
    """Convertir imagen OpenCV a base64"""
    # Mejorar calidad de la imagen
    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), 95]
    _, buffer = cv2.imencode('.jpg', image, encode_param)
    img_base64 = base64.b64encode(buffer).decode('utf-8')
    return img_base64

def apply_image_effects(image):
    """Aplicar efectos a la imagen para generar las 4 variantes - MEJORADO"""
    height, width = image.shape[:2]
    
    # 1. Imagen original con detecci√≥n
    original = image.copy()
    
    # 2. Imagen volteada horizontalmente
    flipped = cv2.flip(image, 1)
    
    # 3. Imagen m√°s brillante y con mejor contraste
    bright = cv2.convertScaleAbs(image, alpha=1.2, beta=25)
    
    # 4. Imagen rotada con mejor calidad
    center = (width // 2, height // 2)
    rotation_matrix = cv2.getRotationMatrix2D(center, 10, 1.0)
    rotated = cv2.warpAffine(image, rotation_matrix, (width, height), 
                            borderMode=cv2.BORDER_REPLICATE)
    
    return {
        'original_image': original,
        'flipped_image': flipped,
        'bright_image': bright,
        'rotated_image': rotated
    }

# Inicializar detector
logger.info("üöÄ Inicializando detector de emociones...")
detector = EmotionDetectorHF()

@app.route('/analyze', methods=['POST'])
def analyze_emotion():
    """Endpoint principal para an√°lisis de emociones - COMPATIBLE CON TU BOT"""
    try:
        logger.info("üì∏ Nueva solicitud de an√°lisis de emociones")
        
        # Verificar que se envi√≥ un archivo (igual que tu c√≥digo)
        if 'file' not in request.files:
            return jsonify({'error': 'No se encontr√≥ archivo'}), 400
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({'error': 'Archivo vac√≠o'}), 400
        
        # Leer imagen
        image_data = file.read()
        nparr = np.frombuffer(image_data, np.uint8)
        image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        
        if image is None:
            return jsonify({'error': 'No se pudo procesar la imagen'}), 400
        
        logger.info(f"üìä Imagen cargada: {image.shape}")
        
        # Procesar imagen y detectar emociones
        processed_image, emotion_results = detector.process_image(image)
        
        # Aplicar efectos para generar las 4 variantes
        image_variants = apply_image_effects(processed_image)
        
        # Convertir todas las im√°genes a base64
        response_data = {}
        for key, img in image_variants.items():
            response_data[key] = image_to_base64(img)
            logger.info(f"‚úÖ {key} convertida a base64")
        
        # Agregar resultados de emociones (EXACTO como espera tu bot)
        response_data['emotions'] = emotion_results
        response_data['total_faces'] = len(emotion_results)
        
        # Obtener emoci√≥n dominante
        if emotion_results:
            dominant_emotion = max(emotion_results, key=lambda x: x['confidence'])
            response_data['dominant_emotion'] = dominant_emotion['emotion']
            response_data['dominant_confidence'] = dominant_emotion['confidence']
        else:
            response_data['dominant_emotion'] = 'No face detected'
            response_data['dominant_confidence'] = 0.0
        
        logger.info(f"üéØ Procesamiento exitoso: {len(emotion_results)} caras detectadas")
        logger.info(f"üèÜ Emoci√≥n dominante: {response_data.get('dominant_emotion', 'N/A')}")
        
        return jsonify(response_data)
        
    except Exception as e:
        logger.error(f"‚ùå Error en an√°lisis: {str(e)}")
        return jsonify({'error': f'Error interno: {str(e)}'}), 500

@app.route('/health', methods=['GET'])
def health_check():
    """Endpoint de verificaci√≥n de salud"""
    return jsonify({
        'status': 'healthy',
        'model_type': 'Hugging Face' if detector.emotion_classifier else 'Local Backup',
        'supported_emotions': EMOTIONS,
        'version': '2.0.0'
    })

@app.route('/', methods=['GET'])
def home():
    """Endpoint de informaci√≥n"""
    return jsonify({
        'message': 'ü§ñ API de Detecci√≥n de Emociones con IA',
        'version': '2.0.0',
        'model': 'Hugging Face Transformers',
        'endpoints': {
            '/analyze': 'POST - Analizar emociones en imagen',
            '/health': 'GET - Estado de la API'
        },
        'usage': 'Env√≠a una imagen usando form-data con key "file"',
        'compatible_with': 'Telegram Bot Architecture'
    })

if __name__ == '__main__':
    # Para desarrollo local
    app.run(debug=True, host='0.0.0.0', port=5000)

# Para producci√≥n (Render/Railway/etc)
# gunicorn app:app